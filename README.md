# NLP Note

## 참고 
- https://kcorpus.korean.go.kr/, https://corpus.korean.go.kr/ (Korean corpus)
- https://wikidocs.net/book/2155 (딥 러닝을 이용한 자연어 처리 입문)
- https://www.kaggle.com/ (kaggle)
- http://www.ontorus.net/page/default.aspx (Korean thesaurus)
- https://github.com/uoneway/Text-Summarization-Repo 텍스트 요약
- https://dacon.io/ 데이콘

## Korean NLP Package
- KoNLPy https://github.com/konlpy/konlpy 형태소 분석, 품사 태깅
- KSS(Korean Sentence Splitter) https://github.com/hyunwoongko/kss 문장 분리
- PyKoSpacing https://github.com/haven-jeon/PyKoSpacing 띄어쓰기
- Py-Hanspell https://github.com/ssut/py-hanspell.git 맞춤법 검사
- soynlp https://github.com/lovit/soynlp 단어 추출, 품사 태깅, 토큰화

## Hot Paper
- [Attention Is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
- [ERNIE: Enhanced Language Representation with Informative Entities](https://aclanthology.org/P19-1139.pdf)

## Transformer, Attention
- https://ahnjg.tistory.com/57
- https://blog.promedius.ai/transformer/

## Note
- csv가 느릴 때는 Parquet 사용
- [Stemming, Lemmatization은 필수인가?](https://inhyeokyoo.github.io/nlp/korean-preprocessing/#lemmatization-stemming)
