# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

- GPT, BERT : 직접 labeling할 필요 x
- GPT-1 => BERT : 단방향의 decoder를 사용한 GPT는 문맥 이해 능력이 떠어질 수 있다.
- Tokenization : WordPiece Embedding
- Position Embedding : sin,cos func (규칙적인 함수, 상대적 위치 전송 가능)
- pre-training => fine tuning 필요

### Referrence
- [[딥러닝 자연어처리] BERT 이해하기](https://www.youtube.com/watch?v=30SvdoA6ApE)
